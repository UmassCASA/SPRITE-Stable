{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef5bcd5-74c1-42c4-afc6-028aad306265",
   "metadata": {},
   "source": [
    "This notebook downloads a full day's woth of rainfall data from the CASA repository and converts it from individual NetCDF files into one pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562b809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xarray\n",
      "  Downloading xarray-2023.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.1.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting pysftp\n",
      "  Downloading pysftp-0.2.9.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy>=1.22 (from xarray)\n",
      "  Using cached numpy-1.26.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/arutkovskii_umass_edu/.conda/envs/casa-venv/lib/python3.9/site-packages (from xarray) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/arutkovskii_umass_edu/.conda/envs/casa-venv/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting paramiko>=1.17 (from pysftp)\n",
      "  Downloading paramiko-3.4.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting bcrypt>=3.2 (from paramiko>=1.17->pysftp)\n",
      "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting cryptography>=3.3 (from paramiko>=1.17->pysftp)\n",
      "  Downloading cryptography-41.0.7-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pynacl>=1.5 (from paramiko>=1.17->pysftp)\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/arutkovskii_umass_edu/.conda/envs/casa-venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting cffi>=1.12 (from cryptography>=3.3->paramiko>=1.17->pysftp)\n",
      "  Downloading cffi-1.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=3.3->paramiko>=1.17->pysftp)\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xarray-2023.12.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.1.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Downloading paramiko-3.4.0-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.9/225.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-41.0.7-cp37-abi3-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.4/443.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pysftp\n",
      "  Building wheel for pysftp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pysftp: filename=pysftp-0.2.9-py3-none-any.whl size=15496 sha256=0852f6e4b68a1fff8b182a0d84069615f1e7abccf429e552c6c3461f417d269b\n",
      "  Stored in directory: /home/arutkovskii_umass_edu/.cache/pip/wheels/0c/0e/4e/b8c1140f0fdcfb73bafe525942bff85043231e6787b7eab72c\n",
      "Successfully built pysftp\n",
      "Installing collected packages: pytz, tzdata, pycparser, numpy, bcrypt, pandas, cffi, xarray, pynacl, cryptography, paramiko, pysftp\n",
      "Successfully installed bcrypt-4.1.2 cffi-1.16.0 cryptography-41.0.7 numpy-1.26.3 pandas-2.1.4 paramiko-3.4.0 pycparser-2.21 pynacl-1.5.0 pysftp-0.2.9 pytz-2023.3.post1 tzdata-2023.4 xarray-2023.12.0\n"
     ]
    }
   ],
   "source": [
    "! pip install xarray pandas pysftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5943f1b8-4ead-4452-96d0-379eae075ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pysftp\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f53a651-e0b1-4a75-94ff-0ebc90888405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_directory(ssh_host, ssh_username, ssh_pk, remote_dir, local_dir):\n",
    "    # Connect to the SFTP server\n",
    "    # The SFTP server is connected to the NAS that holds the CASA data repository\n",
    "    cnopts = pysftp.CnOpts()\n",
    "    cnopts.hostkeys = None\n",
    "    with pysftp.Connection(ssh_host, username=ssh_username, private_key=ssh_pk, cnopts=cnopts) as sftp:\n",
    "        # Change to the remote directory\n",
    "        sftp.chdir(remote_dir)\n",
    "\n",
    "        # List the contents of the remote directory\n",
    "        remote_files = sftp.listdir()\n",
    "\n",
    "        # Recursively download each file\n",
    "        for file_name in remote_files:\n",
    "            remote_path = os.path.join(remote_dir, file_name)\n",
    "            local_path = os.path.join(local_dir, file_name)\n",
    "\n",
    "            # If it's a directory, create the local directory\n",
    "            if sftp.isdir(remote_path):\n",
    "                os.makedirs(local_path, exist_ok=True)\n",
    "                download_directory(ssh_host, ssh_username, ssh_pk, remote_path, local_path)\n",
    "            else:\n",
    "                # Download the file\n",
    "                sftp.get(remote_path, local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa70bdd-c31d-4672-ac05-547252b042f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "ssh_host = \"\"\n",
    "ssh_username = \"\"\n",
    "ssh_pk = \"\"\n",
    "remote_directory = \"/mnt/casa-ssd-pool/casa/qpe/20180908\"\n",
    "local_directory_gz = \"/work/pi_mzink_umass_edu/SPRITE/UsableNotebooks/netcdf/20180908_gz\"\n",
    "local_directory = \"/work/pi_mzink_umass_edu/SPRITE/UsableNotebooks/netcdf/20180908\"\n",
    "\n",
    "if not os.path.exists(local_directory):\n",
    "    os.makedirs(local_directory)\n",
    "\n",
    "if not os.path.exists(local_directory_gz):\n",
    "    os.makedirs(local_directory_gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc702cee-2bdd-4563-96df-5ef76d1270fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_directory(ssh_host, ssh_username, ssh_pk, remote_directory, local_directory_gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad86767-c7b5-4a97-a92d-1453f720a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def netcdf_to_dataframe(directory_path):\n",
    "    # Create an empty list to store individual DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate through each NetCDF file in the directory\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        local_file = os.path.splitext(file_name)[0]\n",
    "        with (\n",
    "            gzip.open(os.path.join(directory_path, file_name), \"rb\") as f_in,\n",
    "            open(os.path.join(directory_path, local_file), \"wb\") as f_out,\n",
    "        ):\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        file_path = os.path.join(directory_path, local_file)\n",
    "\n",
    "        # Open the NetCDF file using xarray\n",
    "        ds = xr.open_dataset(file_path)\n",
    "\n",
    "        # Convert the xarray dataset to a Pandas DataFrame\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(dfs)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4e8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limited_netcdf_to_dataframe(directory_path_gz, unzip_directory_path, number_of_files=10):\n",
    "    # Create an empty list to store individual DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate through each NetCDF file in the directory\n",
    "    for file_name in os.listdir(directory_path_gz)[:number_of_files]:\n",
    "        local_file = os.path.splitext(file_name)[0]\n",
    "        with (\n",
    "            gzip.open(os.path.join(directory_path_gz, file_name), \"rb\") as f_in,\n",
    "            open(os.path.join(unzip_directory_path, local_file), \"wb\") as f_out,\n",
    "        ):\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        file_path = os.path.join(unzip_directory_path, local_file)\n",
    "\n",
    "        # Open the NetCDF file using xarray\n",
    "        ds = xr.open_dataset(file_path)\n",
    "\n",
    "        # Convert the xarray dataset to a Pandas DataFrame\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(dfs)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7ee426-9983-4bfd-a09f-95810969057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_dataframe = netcdf_to_dataframe(local_directory) # run out of memory\n",
    "result_dataframe = limited_netcdf_to_dataframe(local_directory_gz, local_directory, number_of_files=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f187acdd-056e-41b3-a76a-64ba73277899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 128100 entries, (0.5, 31.775, -97.99) to (0.5, 33.6, -96.244995)\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   RRdata  128100 non-null  float32\n",
      "dtypes: float32(1)\n",
      "memory usage: 1.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128100, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dataframe.info()\n",
    "result_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d0205",
   "metadata": {},
   "source": [
    "# This is for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c97b7610-5889-4485-a6fe-9812391f6005",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Draft of a custom dataset class for PyTorch to load the NetCDF files to model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNetCDFDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m file_path\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Draft of a custom dataset class for PyTorch to load the NetCDF files to model\n",
    "\n",
    "\n",
    "class NetCDFDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "        # Open the NetCDF file\n",
    "        os.chdir(local_directory)\n",
    "        netcdfiles = os.listdir()\n",
    "        with nc.Dataset(file_path, \"r\") as nc_file:\n",
    "            self.data = nc_file.variables[\"RRdata\"][:]\n",
    "            self.labels = nc_file.variables[\"labels\"][:]  # Assuming you have labels in your NetCDF file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\"input\": self.data[idx], \"label\": self.labels[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87ff5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casa-venv",
   "language": "python",
   "name": "casa-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
