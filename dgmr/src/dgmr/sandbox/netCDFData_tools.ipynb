{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different methods to download data from CASA, find information about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pysftp\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import shutil\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "ssh_host = \"\"\n",
    "ssh_username = \"\"\n",
    "ssh_pk = \"\"\n",
    "remote_directory = \"/mnt/casa-ssd-pool/casa/qpe/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = \"20180908\"\n",
    "split = \"validation\"  #'train', 'validation', 'test'\n",
    "local_directory_gz = f\"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData/{split}_gz/\"\n",
    "local_directory = f\"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData/{split}/\"\n",
    "\n",
    "os.makedirs(local_directory, exist_ok=True)\n",
    "os.makedirs(local_directory_gz, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "\n",
    "def move_and_resize(local_directory, new_directory):\n",
    "    \"\"\"\n",
    "    Iterate through the each split folder, read iteratively netCDF files,\n",
    "    resize from 366,350 to 256,256, and save in the new directory under the particular day,\n",
    "    with the same name and .nc extension.\n",
    "\n",
    "    Args:\n",
    "    local_directory: str\n",
    "        The directory of a particular split.\n",
    "    new_directory: str\n",
    "        The split directory where the resized netCDF files are saved.\n",
    "    \"\"\"\n",
    "\n",
    "    day_folder = os.listdir(local_directory)\n",
    "\n",
    "    os.makedirs(new_directory, exist_ok=True)\n",
    "\n",
    "    for day in day_folder:\n",
    "        files = os.listdir(os.path.join(local_directory, day))\n",
    "        day_save_dir = os.path.join(new_directory, day)\n",
    "        os.makedirs(day_save_dir, exist_ok=True)\n",
    "\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(local_directory, day, filename)\n",
    "            resized_rr_data = resize(file_path)\n",
    "            save_nc(filename, day_save_dir, resized_rr_data)\n",
    "\n",
    "\n",
    "def resize(file_path):\n",
    "    with Dataset(file_path, \"r\") as nc_data:\n",
    "        # Extract the frame and remove the leading dimension\n",
    "        rr_data = nc_data.variables[\"RRdata\"][:]  # 366, 350, 1\n",
    "        x_data = nc_data.variables[\"x0\"][:]\n",
    "        y_data = nc_data.variables[\"y0\"][:]\n",
    "\n",
    "    filename = os.path.basename(file_path)\n",
    "    no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Trim 8 pixels from top and bottom, then resize to 256x256\n",
    "    trimmed_rr_data = rr_data[:, 8:-8, :]  # (1, 350, 350)\n",
    "    trimmed_y_data = y_data[8:-8]  # (350,)\n",
    "\n",
    "    # Resize to 256, 256\n",
    "    resized_rr_data = cv2.resize(trimmed_rr_data[0], (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Add channel dimension â€“ 256,256 to 1, 256, 256\n",
    "    resized_rr_data = resized_rr_data[np.newaxis, :, :]\n",
    "\n",
    "    return resized_rr_data\n",
    "\n",
    "\n",
    "def save_nc(filename, save_dir, resized_rr_data):\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with Dataset(save_path, \"w\", format=\"NETCDF4\") as nc_data:\n",
    "        # Create dimensions based on the resized data. Assuming resized_rr_data is (1, 256, 256)\n",
    "        nc_data.createDimension(\"x0\", 256)  # Adjusted to resized shape\n",
    "        nc_data.createDimension(\"y0\", 256)  # Adjusted to resized shape\n",
    "        nc_data.createDimension(\"z0\", 1)  # Keeping the original 'time' dimension as 'z0'\n",
    "\n",
    "        # Create variables with dimensions. The dimensions are named to match the original file's structure.\n",
    "        x = nc_data.createVariable(\"x0\", \"f4\", (\"x0\",))\n",
    "        y = nc_data.createVariable(\"y0\", \"f4\", (\"y0\",))\n",
    "        z = nc_data.createVariable(\"z0\", \"f4\", (\"z0\",))\n",
    "        rr = nc_data.createVariable(\"RRdata\", \"f4\", (\"z0\", \"y0\", \"x0\"))\n",
    "\n",
    "        x[:] = np.linspace(start=0, stop=1, num=256)  # Example placeholder values\n",
    "        y[:] = np.linspace(start=0, stop=1, num=256)  # Example placeholder values\n",
    "        z[:] = np.array([0])  # Keeping a placeholder if you don't have a specific value for 'z0'\n",
    "        rr[:] = resized_rr_data\n",
    "\n",
    "\n",
    "# train split\n",
    "train_local_directory = f\"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData/train/\"\n",
    "train_new_directory = f\"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData_processed/train/\"\n",
    "move_and_resize(train_local_directory, train_new_directory)\n",
    "\n",
    "# validation split\n",
    "validation_local_directory = f\"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData/validation/\"\n",
    "validation_new_directory = f\"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData_processed/validation/\"\n",
    "move_and_resize(validation_local_directory, validation_new_directory)\n",
    "\n",
    "# test split\n",
    "test_local_directory = f\"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData/test/\"\n",
    "test_new_directory = f\"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData_processed/test/\"\n",
    "move_and_resize(test_local_directory, test_new_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_info(ssh_host, ssh_username, remote_dir):\n",
    "    \"\"\"\n",
    "    Extract days for splits and total number of frames and batches\n",
    "    \"\"\"\n",
    "    cnopts = pysftp.CnOpts()\n",
    "    cnopts.hostkeys = None\n",
    "\n",
    "    with pysftp.Connection(ssh_host, username=ssh_username, private_key=ssh_pk, cnopts=cnopts) as sftp:\n",
    "        sftp.chdir(remote_dir)\n",
    "        day_folders = sftp.listdir()\n",
    "        day_folders.sort()\n",
    "\n",
    "        total_number_of_frames_downloaded = 0\n",
    "        offset = 0\n",
    "\n",
    "        splits = {\"train\": [False, \"\"], \"validation\": [False, \"\"], \"test\": [False, \"\"]}\n",
    "\n",
    "        # I need to understand how many batches (if we take every 5th frame and there are 22 frames in each batch) we have in total\n",
    "        # I need to know on what day we cross 24000 mark, and then when we cross 29000 mark, and 34000 mark. Remeber each of those marks because we will need to split the data into train, validation, and test sets\n",
    "\n",
    "        for day in day_folders:\n",
    "            day_path = os.path.join(remote_dir, day)\n",
    "\n",
    "            files = [f for f in sftp.listdir(day_path) if f.endswith(\".gz\")]\n",
    "            number_of_frames = len(files)\n",
    "            frames_to_download_indices = [\n",
    "                (x * 5 + offset) % number_of_frames for x in range((number_of_frames + offset) // 5)\n",
    "            ]\n",
    "\n",
    "            total_number_of_frames_downloaded += len(frames_to_download_indices)\n",
    "            total_number_batches = total_number_of_frames_downloaded // 22\n",
    "\n",
    "            if total_number_batches >= 24000 and not splits[\"train\"][0]:\n",
    "                splits[\"train\"] = [True, day]\n",
    "            if total_number_batches >= 29000 and not splits[\"validation\"][0]:\n",
    "                splits[\"validation\"] = [True, day]\n",
    "            if total_number_batches >= 34000 and not splits[\"test\"][0]:\n",
    "                splits[\"test\"] = [True, day]\n",
    "\n",
    "            # Update the offset for the next day\n",
    "            offset = (offset + number_of_frames) % 5\n",
    "\n",
    "        # Get the Info\n",
    "        for split, ar in splits.items():\n",
    "            print(f\"Split: {split}, End Day: {ar[1]}\")\n",
    "\n",
    "        print(f\"Total number of frames: {total_number_of_frames_downloaded}\")\n",
    "        print(f\"Total number of batches: {total_number_batches}\")\n",
    "\n",
    "\n",
    "find_info(ssh_host, ssh_username, remote_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_split(ssh_host, ssh_username, remote_dir, local_dir_gz, local_dir, start_day, end_day):\n",
    "    \"\"\"Download the split data based on a range from the remote directory to the local directory\"\"\"\n",
    "\n",
    "    cnopts = pysftp.CnOpts()\n",
    "    cnopts.hostkeys = None\n",
    "    total_number_of_frames_downloaded = 0\n",
    "    offset = 0  # Variable to manage the offset for downloading every 5th frame across days\n",
    "\n",
    "    with pysftp.Connection(ssh_host, username=ssh_username, private_key=ssh_pk, cnopts=cnopts) as sftp:\n",
    "        sftp.chdir(remote_dir)\n",
    "        day_folders = sftp.listdir()\n",
    "        day_folders.sort()\n",
    "\n",
    "        if end_day:\n",
    "            day_folders = [day for day in day_folders if start_day <= day < end_day]\n",
    "\n",
    "        else:\n",
    "            day_folders = [day for day in day_folders if start_day <= day]\n",
    "\n",
    "        for day_folder in day_folders:\n",
    "            files = [f for f in sftp.listdir(os.path.join(remote_dir, day_folder)) if f.endswith(\".gz\")]\n",
    "            number_of_frames = len(files)\n",
    "            # Download every 5th frame with an offset\n",
    "            frames_to_download_indices = [\n",
    "                (x * 5 + offset) % number_of_frames for x in range((number_of_frames + offset) // 5)\n",
    "            ]\n",
    "\n",
    "            print(f\"Downloading: {day_folder}\\nNumber of frames: {len(frames_to_download_indices)}\")\n",
    "            remote_path_day = os.path.join(remote_dir, day_folder)\n",
    "            local_path_gz_day = os.path.join(local_dir_gz, day_folder)\n",
    "            local_path_day = os.path.join(local_dir, day_folder)\n",
    "\n",
    "            if sftp.isdir(remote_path_day):\n",
    "                os.makedirs(local_path_gz_day, exist_ok=True)\n",
    "                os.makedirs(local_path_day, exist_ok=True)\n",
    "\n",
    "                for index in frames_to_download_indices:\n",
    "                    file_name = files[index]\n",
    "                    remote_path_file = os.path.join(remote_path_day, file_name)\n",
    "                    local_path_gz_file = os.path.join(local_path_gz_day, file_name)\n",
    "                    local_path_file = os.path.join(local_path_day, os.path.splitext(file_name)[0])\n",
    "                    sftp.get(remote_path_file, local_path_gz_file)\n",
    "\n",
    "                    with gzip.open(local_path_gz_file, \"rb\") as f_in, open(local_path_file, \"wb\") as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "                    os.remove(local_path_gz_file)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"The remote path is not a directory\")\n",
    "\n",
    "            total_number_of_frames_downloaded += len(frames_to_download_indices)\n",
    "            # Update the offset for the next day\n",
    "            offset = (offset + number_of_frames) % 5\n",
    "\n",
    "    print(f\"Total number of frames downloaded: {total_number_of_frames_downloaded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def download_and_extract_split(\n",
    "    ssh_host, ssh_username, remote_directory, local_directory_base, split, start_day=\"\", end_day=\"\"\n",
    "):\n",
    "    local_directory_gz = os.path.join(local_directory_base, f\"{split}_gz/\")\n",
    "    local_directory = os.path.join(local_directory_base, split)\n",
    "\n",
    "    os.makedirs(local_directory, exist_ok=True)\n",
    "    os.makedirs(local_directory_gz, exist_ok=True)\n",
    "\n",
    "    download_split(ssh_host, ssh_username, remote_directory, local_directory_gz, local_directory, start_day, end_day)\n",
    "\n",
    "    shutil.rmtree(local_directory_gz)\n",
    "\n",
    "\n",
    "# Training split\n",
    "download_and_extract_split(\n",
    "    ssh_host,\n",
    "    ssh_username,\n",
    "    remote_directory,\n",
    "    \"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData/\",\n",
    "    \"train\",\n",
    "    \"20160301\",\n",
    "    \"20210913\",\n",
    ")\n",
    "\n",
    "# Validation split\n",
    "download_and_extract_split(\n",
    "    ssh_host,\n",
    "    ssh_username,\n",
    "    remote_directory,\n",
    "    \"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData/\",\n",
    "    \"validation\",\n",
    "    \"20210913\",\n",
    "    \"20221221\",\n",
    ")\n",
    "\n",
    "# Test split\n",
    "download_and_extract_split(\n",
    "    ssh_host,\n",
    "    ssh_username,\n",
    "    remote_directory,\n",
    "    \"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData/\",\n",
    "    \"test\",\n",
    "    \"20221221\",\n",
    "    \"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(\n",
    "    ssh_host, ssh_username, remote_dir, local_dir_gz, local_dir, start_day=\"20180908\", all=False, number_of_days=2\n",
    "):\n",
    "    cnopts = pysftp.CnOpts()\n",
    "    cnopts.hostkeys = None\n",
    "    total_number_of_frames_downloaded = 0\n",
    "    offset = 0  # Variable to manage the offset for downloading every 5th frame across days\n",
    "\n",
    "    with pysftp.Connection(ssh_host, username=ssh_username, private_key=ssh_pk, cnopts=cnopts) as sftp:\n",
    "        sftp.chdir(remote_dir)\n",
    "        day_folders = sftp.listdir()\n",
    "        day_folders.sort()\n",
    "        day_folders = [day for day in day_folders if day >= start_day]\n",
    "\n",
    "        # Until today\n",
    "        if all:\n",
    "            number_of_days = len(day_folders)\n",
    "\n",
    "        for i, day_folder in enumerate(day_folders):\n",
    "            if i < number_of_days:\n",
    "                files = [f for f in sftp.listdir(os.path.join(remote_dir, day_folder)) if f.endswith(\".gz\")]\n",
    "                number_of_frames = len(files)\n",
    "                # Download every 5th frame with an offset\n",
    "                frames_to_download_indices = [\n",
    "                    (x * 5 + offset) % number_of_frames for x in range((number_of_frames + offset) // 5)\n",
    "                ]\n",
    "\n",
    "                print(f\"Downloading: {day_folder}\\nNumber of frames: {len(frames_to_download_indices)}\")\n",
    "                remote_path_day = os.path.join(remote_dir, day_folder)\n",
    "                local_path_gz_day = os.path.join(local_dir_gz, day_folder)\n",
    "                local_path_day = os.path.join(local_dir, day_folder)\n",
    "\n",
    "                if sftp.isdir(remote_path_day):\n",
    "                    os.makedirs(local_path_gz_day, exist_ok=True)\n",
    "                    os.makedirs(local_path_day, exist_ok=True)\n",
    "\n",
    "                    for index in frames_to_download_indices:\n",
    "                        file_name = files[index]\n",
    "                        remote_path_file = os.path.join(remote_path_day, file_name)\n",
    "                        local_path_gz_file = os.path.join(local_path_gz_day, file_name)\n",
    "                        local_path_file = os.path.join(local_path_day, os.path.splitext(file_name)[0])\n",
    "                        sftp.get(remote_path_file, local_path_gz_file)\n",
    "\n",
    "                        with gzip.open(local_path_gz_file, \"rb\") as f_in, open(local_path_file, \"wb\") as f_out:\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "                        os.remove(local_path_gz_file)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(\"The remote path is not a directory\")\n",
    "\n",
    "                total_number_of_frames_downloaded += len(frames_to_download_indices)\n",
    "                # Update the offset for the next day\n",
    "                offset = (offset + number_of_frames) % 5\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    print(f\"Total number of frames downloaded: {total_number_of_frames_downloaded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(\n",
    "    ssh_host, ssh_username, remote_directory, local_directory_gz, local_directory, start_day=\"20230101\", all=True\n",
    ")\n",
    "shutil.rmtree(local_directory_gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a particular file\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import gzip\n",
    "import shutil\n",
    "import pysftp\n",
    "\n",
    "\n",
    "cnopts = pysftp.CnOpts()\n",
    "cnopts.hostkeys = None\n",
    "\n",
    "with pysftp.Connection(ssh_host, username=ssh_username, private_key=ssh_pk, cnopts=cnopts) as sftp:\n",
    "    sftp.chdir(remote_directory)\n",
    "    file_path = \"./20160301/20160301_053157.nc.gz\"\n",
    "\n",
    "    local_dir = \"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData/verify/20160301\"\n",
    "\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    local_path_gz_file = os.path.join(local_dir, os.path.basename(file_path))\n",
    "    local_path_file = os.path.splitext(local_path_gz_file)[0]\n",
    "\n",
    "    sftp.get(file_path, local_path_gz_file)\n",
    "\n",
    "    with gzip.open(local_path_gz_file, \"rb\") as f_in, open(local_path_file, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(local_path_gz_file)\n",
    "\n",
    "\n",
    "with Dataset(\n",
    "    local_path_file,\n",
    "    \"r\",\n",
    ") as nc_data:\n",
    "    # Set auto mask to False\n",
    "    nc_data.set_auto_mask(False)\n",
    "    # Extract the frame and remove the leading dimension\n",
    "    rr_data = nc_data.variables[\"RRdata\"][:]  # 366, 350, 1\n",
    "    x_data = nc_data.variables[\"x0\"][:]\n",
    "    y_data = nc_data.variables[\"y0\"][:]\n",
    "\n",
    "\n",
    "if isinstance(rr_data, np.ma.MaskedArray):\n",
    "    print(\"The data is masked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what is the first and last day in the remote directory\n",
    "def get_first_last_day(ssh_host, ssh_username, remote_dir):\n",
    "    cnopts = pysftp.CnOpts()\n",
    "    cnopts.hostkeys = None\n",
    "    with pysftp.Connection(ssh_host, username=ssh_username, private_key=ssh_pk, cnopts=cnopts) as sftp:\n",
    "        sftp.chdir(remote_dir)\n",
    "        day_folders = sftp.listdir()\n",
    "        day_folders.sort()\n",
    "        return day_folders[0], day_folders[-1]\n",
    "\n",
    "\n",
    "first_day, last_day = get_first_last_day(ssh_host, ssh_username, remote_directory)\n",
    "print(first_day, last_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To approximate the number of frames and sequences for the entire dataset for training\n",
    "\n",
    "cnopts = pysftp.CnOpts()\n",
    "cnopts.hostkeys = None\n",
    "stop_day = \"20230101\"\n",
    "\n",
    "with pysftp.Connection(\n",
    "    Config.CASA_SSH_HOST, username=Config.CASA_SSH_USERNAME, private_key=Config.CASA_PRIVATE_KEY, cnopts=cnopts\n",
    ") as sftp:\n",
    "    sftp.chdir(Config.REMOTE_DIR)\n",
    "\n",
    "    all_days = sftp.listdir()\n",
    "    all_days = [day for day in all_days if day <= stop_day]\n",
    "\n",
    "    all_files = []\n",
    "\n",
    "    for day in all_days:\n",
    "        sftp.chdir(day)\n",
    "        all_files.extend(sftp.listdir())\n",
    "        sftp.chdir(\"..\")\n",
    "\n",
    "    number_of_frames = len(all_files) // 5\n",
    "    number_of_sequences = number_of_frames // TOTAL_FRAMES\n",
    "\n",
    "print(f\"Number of frames: {number_of_frames}\")\n",
    "print(f\"Number of sequences: {number_of_sequences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DELETE THE TRAIN AND VALIDATION FOLDERS\n",
    "\n",
    "import shutil\n",
    "import traceback\n",
    "\n",
    "\n",
    "def remove(path):\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        remove(path)\n",
    "\n",
    "\n",
    "remove(\"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData/validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the processed data contains Nans after disabling the set_auto_mask to false for the netCDF4\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "\n",
    "with Dataset(\n",
    "    \"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData_processed/train/20160301/20160301_053157.nc\", \"r\"\n",
    ") as nc_data:\n",
    "    nc_data.set_auto_mask(False)\n",
    "    # Extract the frame and remove the leading dimension\n",
    "    rr_data = nc_data.variables[\"RRdata\"][:]  # 366, 350, 1\n",
    "    x_data = nc_data.variables[\"x0\"][:]\n",
    "    y_data = nc_data.variables[\"y0\"][:]\n",
    "\n",
    "\n",
    "# check if rr_data is MaskedArray\n",
    "if isinstance(rr_data, np.ma.MaskedArray):\n",
    "    print(\"Masked Array\")\n",
    "\n",
    "# check if there are any nans\n",
    "if np.isnan(rr_data).any():\n",
    "    print(\"Nans\")\n",
    "\n",
    "\n",
    "# Repeat for not processed data\n",
    "with Dataset(\n",
    "    \"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/netCDFData/train/20160301/20160301_053157.nc\", \"r\"\n",
    ") as nc_data:\n",
    "    nc_data.set_auto_mask(False)\n",
    "    # Extract the frame and remove the leading dimension\n",
    "    rr_data = nc_data.variables[\"RRdata\"][:]  # 366, 350, 1\n",
    "    x_data = nc_data.variables[\"x0\"][:]\n",
    "    y_data = nc_data.variables[\"y0\"][:]\n",
    "\n",
    "# check if rr_data is MaskedArray\n",
    "if isinstance(rr_data, np.ma.MaskedArray):\n",
    "    print(\"Masked Array\")\n",
    "\n",
    "# check if there are any nans\n",
    "if np.isnan(rr_data).any():\n",
    "    print(\"Nans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "file_path = \"/work/pi_mzink_umass_edu/SPRITE/skillful_nowcasting/output/sorted_max_values.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'max_value1' to numeric, coercing errors to NaN\n",
    "df[\"max_value1\"] = pd.to_numeric(df[\"max_value1\"], errors=\"coerce\")\n",
    "\n",
    "# Define ranges, including open-ended ranges above 300\n",
    "ranges = [\n",
    "    (128, float(\"inf\")),\n",
    "    (135, 140),\n",
    "    (140, 150),\n",
    "    (150, 160),\n",
    "    (160, 170),\n",
    "    (170, 180),\n",
    "    (180, 190),\n",
    "    (190, 200),\n",
    "    (200, 210),\n",
    "    (210, 220),\n",
    "    (220, 230),\n",
    "    (230, 240),\n",
    "    (240, 250),\n",
    "    (250, 260),\n",
    "    (260, 270),\n",
    "    (270, 280),\n",
    "    (280, 290),\n",
    "    (290, 300),\n",
    "    (300, float(\"inf\")),\n",
    "]\n",
    "\n",
    "range_counts = {}\n",
    "\n",
    "for low, high in ranges:\n",
    "    if high == float(\"inf\"):\n",
    "        filtered_df = df[df[\"max_value1\"] > low]\n",
    "        range_name = f\"above {low}\"\n",
    "    else:\n",
    "        filtered_df = df[(df[\"max_value1\"] > low) & (df[\"max_value1\"] < high)]\n",
    "        range_name = f\"between {low} and {high}\"\n",
    "\n",
    "    range_counts[range_name] = len(filtered_df)\n",
    "\n",
    "# Get the total number of rows in the CSV file\n",
    "number_of_rows = len(df)\n",
    "\n",
    "# Print the results\n",
    "for range_name, count in range_counts.items():\n",
    "    print(f\"Number of rows {range_name}: {count}\")\n",
    "\n",
    "# The maximum value in the 'max_value1' column\n",
    "max_value = df[\"max_value1\"].max()\n",
    "print(f\"Maximum value: {max_value}\")\n",
    "\n",
    "print(f\"Total number of rows: {number_of_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgmr-casa-venv",
   "language": "python",
   "name": "dgmr-casa-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
